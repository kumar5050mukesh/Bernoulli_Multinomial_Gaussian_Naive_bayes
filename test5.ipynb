{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45999999999999996"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "\n",
    "\n",
    "# Probability that an employee is a smoker given that he/she uses the health insurance plan\n",
    "# P(Smoker | Health Insurance Plan)\n",
    "\n",
    "\n",
    "# p(H) = 0.7\n",
    "# p(S/H) = 0.4\n",
    "# p(S'/H') = 0.6\n",
    "# p(H')= 1 - 0.7=0.3\n",
    "# p(S) = (p(S/H)* p(H)) + (p(S'/H') * p(H'))\n",
    "# p(H/S) = (p(S/H) * p(H)) / p(S)\n",
    "\n",
    "p_s=0.4*0.7+0.6*0.3\n",
    "p_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086956521739131"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_h_given_s=0.4*0.7/p_s\n",
    "p_h_given_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\"\"\"\n",
    "\n",
    "Bernoulli Naive Bayes is used when the features are binary 0 or 1, such as the presence or absence of a word in a document. \n",
    " commonly used in text classification, spam filtering, and sentiment analysis.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is used when the features are discrete counts, such as the frequency of a word \n",
    "in a document. This type of Naive Bayes is commonly used in text classification, where the frequency of words in a document\n",
    " is used as features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does Bernoulli Naive Bayes handle missing values?\n",
    "\"\"\" Bernoulli Naive Bayes, missing values are usually handled by replacing them with the mean or median value .\n",
    "\n",
    " some variants of Bernoulli Naive Bayes allow for the use of imputation methods to replace missing values with \n",
    "estimates based on other features in the dataset. For example, if two features are strongly correlated, one feature can be used\n",
    " to predict the value of the other feature in cases where it is missing.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\"\"\"Gaussian Naive Bayes can be used for multi-class classification. It can be extended to handle more than two classes\n",
    " by using a one-vs-all or one-vs-one approach. In the one-vs-all approach, the classifier trains one binary classifier per \n",
    " class and makes a prediction based on the highest probability output by any of the classifiers. In the one-vs-one approach, \n",
    " the classifier trains one binary classifier for each pair of classes and makes a prediction based on the class that wins the \n",
    " most pairwise comparisons.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[600 222]\n",
      " [ 15 543]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.73      0.84       822\n",
      "           1       0.71      0.97      0.82       558\n",
      "\n",
      "    accuracy                           0.83      1380\n",
      "   macro avg       0.84      0.85      0.83      1380\n",
      "weighted avg       0.87      0.83      0.83      1380\n",
      "\n",
      "0.8282608695652174\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"spambase.data\")\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb=GaussianNB()\n",
    "\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred=gnb.predict(X_test)\n",
    "y_pred\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[445  86]\n",
      " [111 279]]\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       531\n",
      "           1       0.76      0.72      0.74       390\n",
      "\n",
      "    accuracy                           0.79       921\n",
      "   macro avg       0.78      0.78      0.78       921\n",
      "weighted avg       0.79      0.79      0.79       921\n",
      "\n",
      "Accuracy score 0.7861020629750272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "df = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification report')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[499  32]\n",
      " [ 78 312]]\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       531\n",
      "           1       0.91      0.80      0.85       390\n",
      "\n",
      "    accuracy                           0.88       921\n",
      "   macro avg       0.89      0.87      0.88       921\n",
      "weighted avg       0.88      0.88      0.88       921\n",
      "\n",
      "Accuracy score 0.8805646036916395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "df = pd.read_csv('spambase.data', header=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=42)\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification report')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The results obtained from training and evaluating Naive Bayes classifiers on the Spambase dataset show that the Bernoulli\n",
    " Naive Bayes classifier outperformed the Multinomial Naive Bayes classifier in terms of accuracy, precision, recall, and F1-score.\n",
    "  The accuracy score of the Bernoulli Naive Bayes classifier was 88.75%, while that of the Multinomial Naive Bayes classifier was\n",
    "   84.94%.\n",
    "\n",
    "There are several reasons why the Bernoulli Naive Bayes classifier performed better than the Multinomial Naive Bayes classifier\n",
    " on this dataset. Firstly, the Spambase dataset is a binary classification problem where each feature is a binary indicator of \n",
    " whether a certain word or character is present in the email or not. Therefore, the Bernoulli Naive Bayes classifier, which is \n",
    " designed for binary features, is a more natural fit for this dataset than the Multinomial Naive Bayes classifier, which is\n",
    "  designed for count-based features\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
